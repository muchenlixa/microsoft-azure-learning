{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The first stage of data integration work is to ingest source data from any number of systems into a destination, or an intermediary data store. ADF offers naerly a hundred different data connectors to ingest data. Regardless of which method you choose, you must set up the appropriate infrastruture to support the data ingestion method through integration runtimes. Additionally, you must also make sure that the data ingestion is secure in each data store, and whilst in transt. \n",
    "\n",
    "## List the data factory ingestion methods\n",
    "\n",
    "ADF can accommodate organizations that are embarking on data integration projects from the differing starting point. \n",
    "\n",
    "Typically, many data integration workflows must consider existing pipelines that have been created on previous porjects, with different dependencies and using different technologies. To that end, there are various ingestion methods that can be used to extract data from a variety of sources. \n",
    "\n",
    "## Ingesting data using the Copy Activity\n",
    "\n",
    "This method offers code-free data ingestion pipelines that don't require any transformation during the extraction of the data. Suitable for any greenfield projects that have a simple method of extraction to an intermediary data store. An example of ingesting data using the Copy Activity can include extracting data from multiple source database systems and outputting the data to files in a data lake store. The benefit of this ingestion method is that they are simple to create, but they are not able to deal with sophisticated transformations or business logic. \n",
    "\n",
    "## Ingesting data using Compute Resources\n",
    "\n",
    "ADF can call on compute resources to process data by a data platform service that may be better suited to the job. A great example of this is that ADF can create a pipeline to an analytical data platform such as Spark pools in an Azure Synapse Analytics instance to perform a complex calculation, which generates new data. This data is then ingested back into the pipeline for further downstream processing. There are wide range of computing resource, and the associated activities that they can perform as shown in the following table:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
